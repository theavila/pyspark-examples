{"cells":[{"cell_type":"markdown","source":["##Spark DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["###Imports\n\nFor these examples, we just need to import two **pyspark.sql** libraries:\n- `types`\n- `functions`\n\nWe need `pyspark.sql.types` to define schemas for the DataFrames. The `pyspark.sql.functions` library contains all of the functions specific to SQL and DataFrames in **PySpark**."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *  # Necessary for creating schemas\nfrom pyspark.sql.functions import * # Importing PySpark functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###Creating DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Tuple List"],"metadata":{}},{"cell_type":"code","source":["# Make a tuple list\na_list = [('a', 1), ('b', 2), ('c', 3)]\n\n# Create a Spark DataFrame, without supplying a schema value\ndf_from_list_no_schema = \\\nsqlContext.createDataFrame(a_list)\n\n# Print the DF object\nprint df_from_list_no_schema\n\n# Print a collected list of Row objects\nprint df_from_list_no_schema.collect()\n\n# Show the DataFrame\ndf_from_list_no_schema.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Tuple List and a Schema"],"metadata":{}},{"cell_type":"code","source":["# Create a Spark DataFrame, this time with schema\ndf_from_list_with_schema = \\\nsqlContext.createDataFrame(a_list, ['letters', 'numbers']) # this simple schema contains just column names\n\n# Show the DataFrame\ndf_from_list_with_schema.show()\n\n# Show the DataFrame's schema\ndf_from_list_with_schema.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Making a Simple DataFrame from a Dictionary"],"metadata":{}},{"cell_type":"code","source":["# Make a dictionary\na_dict = [{'letters': 'a', 'numbers': 1},\n          {'letters': 'b', 'numbers': 2},\n          {'letters': 'c', 'numbers': 3}]\n\n# Create a Spark DataFrame from the dictionary\ndf_from_dict = \\\n(sqlContext\n .createDataFrame(a_dict)) # You will get a warning about this\n\n# Show the DataFrame\ndf_from_dict.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Making a Simple DataFrame Using a StructType Schema + RDD"],"metadata":{}},{"cell_type":"code","source":["# Define the schema\nschema = StructType([\n    StructField('letters', StringType(), True),\n    StructField('numbers', IntegerType(), True)])\n\n# Create an RDD from a list\nrdd = sc.parallelize(a_list)\n\n# Create the DataFrame from these raw components\nnice_df = \\\n(sqlContext\n .createDataFrame(rdd, schema))\n\n# Show the DataFrame\nnice_df.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Define the schema\nschema = StructType([\n    StructField('letters', StringType(), True),\n    StructField('numbers', IntegerType(), True)])\n\n# Create an RDD from a list\nrdd = sc.parallelize(a_list)\n\n# Create the DataFrame from these raw components\nnice_df = \\\n(sqlContext\n .createDataFrame(rdd, schema))\n\n# Show the DataFrame\nnice_df.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["###Simple Inspection Functions\n\nWe now have a `nice_df`, here are some nice functions for inspecting the DataFrame."],"metadata":{}},{"cell_type":"code","source":["# `columns`: return all column names as a list\nnice_df.columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# `dtypes`: get the datatypes for all columns\nnice_df.dtypes"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# `printSchema()`: prints the schema of the supplied DF\nnice_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# `schema`: returns the schema of the provided DF as `StructType` schema\nnice_df.schema"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# `first()` returns the first row as a Row while\n# `head()` and `take()` return `n` number of Row objects\nprint nice_df.first() # can't supply a value; never a list\nprint nice_df.head(2) # can optionally supply a value (default: 1);\n                      # with n > 1, a list\nprint nice_df.take(2) # expects a value; always a list"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# `count()`: returns a count of all rows in DF\nnice_df.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# `describe()`: print out stats for numerical columns\nnice_df.describe().show() # can optionally supply a list of column names"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# the `explain()` function explains the under-the-hood evaluation process\nnice_df.explain()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["###Relatively Simple DataFrame Manipulation Functions\n\nLet's use these functions:\n- `unionAll()`: combine two DataFrames together\n- `orderBy()`: perform sorting of DataFrame columns\n- `select()`: select which DataFrame columns to retain\n- `drop()`: select a single DataFrame column to remove\n- `filter()`: retain DataFrame rows that match a condition"],"metadata":{}},{"cell_type":"code","source":["# Take the DataFrame and add it to itself\n(nice_df\n .unionAll(nice_df)\n .show())\n\n# Add it to itself twice\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .show())\n\n# Coercion will occur if schemas don't align\n(nice_df\n .select(['numbers', 'letters'])\n .unionAll(nice_df)\n .show())\n\n(nice_df\n .select(['numbers', 'letters'])\n .unionAll(nice_df)\n .printSchema())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Sorting the DataFrame by the `numbers` column\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .orderBy('numbers')\n .show())\n\n# Sort the same column in reverse order\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df)\n .orderBy('numbers',\n          ascending = False)\n .show())"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# `select()` and `drop()` both take a list of column names\n# and these functions do exactly what you might expect\n\n# Select only the first column of the DF\n(nice_df\n .select('letters')\n .show())\n\n# Re-order columns in the DF using `select()`\n(nice_df\n .select(['numbers', 'letters'])\n .show())\n\n# Drop the second column of the DF\n(nice_df\n .drop('letters')\n .show())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# The `filter()` function performs filtering of DF rows\n\n# Here is some numeric filtering with comparison operators\n# (>, <, >=, <=, ==, != all work)\n\n# Filter rows where values in `numbers` is > 1\n(nice_df\n .filter(nice_df.numbers > 1)\n .show())\n\n# Perform two filter operations\n(nice_df\n .filter(nice_df.numbers > 1)\n .filter(nice_df.numbers < 3)\n .show())\n\n# Not just numbers! Use the `filter()` + `isin()`\n# combo to filter on string columns with a set of values\n(nice_df\n .filter(nice_df.letters\n         .isin(['a', 'b']))\n .show())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["###The 'groupBy' Function and Aggregations\n\nThe `groupBy()` function groups the DataFrame using the specified columns, then, we can run aggregation on them. The available aggregate functions are:\n\n- `count()`: counts the number of records for each group\n- `sum()`: compute the sum for each numeric column for each group\n- `min()`: computes the minimum value for each numeric column for each group\n- `max()`: computes the maximum value for each numeric column for each group\n- `avg()` or `mean()`: computes average values for each numeric columns for each group\n- `pivot()`: pivots a column of the current DataFrame and perform the specified aggregation\n\nBefore we get into aggregations, let's load in a **CSV** with interesting data and create a new DataFrame.\n\nYou do this with the `spark-csv` package. Documentation on that is available at:\n- https://github.com/databricks/spark-csv\n\nThe dataset that will be loaded in to demonstrate contains data about flights departing New York City airports (`JFK`, `LGA`, `EWR`) in 2013. It has 336,776 rows and 16 columns."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object...\nnycflights_schema = StructType([\n  StructField('year', IntegerType(), True),\n  StructField('month', IntegerType(), True),\n  StructField('day', IntegerType(), True),\n  StructField('dep_time', StringType(), True),\n  StructField('dep_delay', IntegerType(), True),\n  StructField('arr_time', StringType(), True),\n  StructField('arr_delay', IntegerType(), True),\n  StructField('carrier', StringType(), True),\n  StructField('tailnum', StringType(), True),\n  StructField('flight', StringType(), True),  \n  StructField('origin', StringType(), True),\n  StructField('dest', StringType(), True),\n  StructField('air_time', IntegerType(), True),\n  StructField('distance', IntegerType(), True),\n  StructField('hour', IntegerType(), True),\n  StructField('minute', IntegerType(), True)\n  ])\n\n# ...and then read the CSV with the schema\nnycflights = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(nycflights_schema)\n .options(header = True)\n .load('/mnt/data-files/nycflights13.csv'))\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Have a look at the schema for the imported dataset\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Have a look at the `nycflights` DataFrame with the `display()` function (available for Databricks Cloud notebooks)\ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Let's group and aggregate\n\n# `groupBy()` will group one or more DF columns\n# and prep them for aggregration functions\n(nycflights\n .groupby('month') # creates 'GroupedData'\n .count() # creates a new column with aggregate `count` values\n .show())\n\n# Use the `agg()` function to perform multiple\n# aggregations\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'avg', 'arr_delay': 'avg'}) # note the new column names\n .show())\n\n# Caveat: you can't perform multiple aggregrations\n# on the same column (only the last is performed)\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'min', 'dep_delay': 'max'})\n .show())"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Use `groupBy()` with a few columns, then aggregate\ndisplay(\n  nycflights\n  .groupby(['month', 'origin', 'dest']) # group by these unique combinations\n  .count()                              # perform a 'count' aggregation on the groups\n  .orderBy(['month', 'count'],\n           ascending = [1, 0])          # order by `month` ascending, `count` descending\n) "],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Use `groupBy()` + `pivot()` + an aggregation function to\n# make a pivot table!\n\n# Get a table of flights by month for each carrier\ndisplay(\n  nycflights\n  .groupBy('month') # group the data for aggregation by `month` number\n  .pivot('carrier') # provide columns of data by `carrier` abbreviation\n  .count()          # create aggregations as a count of rows\n)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Another pivot table idea: get the average departure\n# delay for each carrier at the different NYC airports\ndisplay(\n  nycflights\n  .groupBy('carrier')\n  .pivot('origin')\n  .avg('dep_delay')\n)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["###Column Operations\n\n`Column` instances can be created by:\n\n(1) Selecting a column from a DataFrame\n- `df.colName`\n- `df[\"colName\"]`\n- `df.select(df.colName)`\n- `df.withColumn(df.colName)`\n\n(2) Creating one from an expression\n- `df.colName + 1`\n- `1 / df.colName`\n\nOnce you have a `Column` instance, you can apply a wide range of functions. Some of the functions covered here are:\n- `format_number()`: apply formatting to a number, rounded to `d` decimal places, and return the result as a string\n- `when()` & `otherwise()`: `when()` evaluates a list of conditions and returns one of multiple possible result expressions; if `otherwise()` is not invoked, `None` is returned for unmatched conditions\n- `concat_ws()`: concatenates multiple input string columns together into a single string column, using the given separator\n- `to_utc_timestamp()`: assumes the given timestamp is in given timezone and converts to UTC\n- `year()`: extracts the year of a given date as integer\n- `month()`: extracts the month of a given date as integer\n- `dayofmonth()`: extracts the day of the month of a given date as integer\n- `hour()`: extract the hour of a given date as integer\n- `minute()`: extract the minute of a given date as integer"],"metadata":{}},{"cell_type":"code","source":["# Perform 2 different aggregations, rename those new columns,\n# then do some rounding of the aggregrate values\ndisplay(\n  nycflights\n  .groupby('month')\n  .agg({'dep_delay': 'avg', 'arr_delay': 'avg'})\n  .withColumnRenamed('avg(arr_delay)', 'mean_arr_delay')\n  .withColumnRenamed('avg(dep_delay)', 'mean_dep_delay')\n  .withColumn('mean_arr_delay', format_number('mean_arr_delay', 1))\n  .withColumn('mean_dep_delay', format_number('mean_dep_delay', 1))\n)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Add a new column (`far_or_near`) with a string based on a comparison\n# on a numeric column; uses: `withColumn()`, `when()`, and `otherwise()`\ndisplay(\n  nycflights\n  .withColumn('far_or_near',\n              when(nycflights.distance > 1000, 'far') # the `if-then` statement\n              .otherwise('near'))                     # the `else` statement\n)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Perform a few numerical computations across columns\ndisplay(\n  nycflights\n  .withColumn('dist_per_minute',\n              nycflights.distance / nycflights.air_time) # create new column with division of values\n  .withColumn('dist_per_minute',\n              format_number('dist_per_minute', 2))       # round that new column's float value to 2 decimal places\n  .drop('distance') # drop the `distance` column\n  .drop('air_time') # drop the `air_time` column\n)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Create a proper timestamp for once in your life...\n# We have all the components: `year`, `month`, `day`,\n# `hour`, and `minute`\n\n# Use `concat_ws()` (concatentate with separator) to\n# combine column data into StringType columns such\n# that dates (`-` separator, YYYY-MM-DD) and times\n# (`:` separator, 24-hour time) are formed\nnycflights = \\\n(nycflights\n .withColumn('date',\n             concat_ws('-',\n                       nycflights.year,\n                       nycflights.month,\n                       nycflights.day))\n .withColumn('time',\n             concat_ws(':',\n                       nycflights.hour,\n                       nycflights.minute)))\n\n# In a second step, concatenate with `concat_ws()`\n# the `date` and `time` strings (separator is a space);\n# then drop several columns\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             concat_ws(' ',\n                       nycflights.date,\n                       nycflights.time))\n .drop('year')     # `drop()` doesn't accept\n .drop('month')    # a list of column names,\n .drop('day')      # therefore, for every column\n .drop('hour')     # we would like to remove\n .drop('minute')   # from the DataFrame, we \n .drop('date')     # must create a new `drop()`\n .drop('time'))    # statement\n\n# In the final step, convert the `timestamp` from\n# a StringType into a TimestampType\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             to_utc_timestamp(nycflights.timestamp, 'GMT')))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Display the `nycflights` DataFrame with the new `timestamp` column\ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# It probably doesn't matter in the end, but,\n# I'd prefer that the `timestamp` column be\n# the first column; let's make use of the\n# `columns` method and get slicing!\nnycflights = \\\n (nycflights\n  .select(nycflights.columns[-1:] + nycflights.columns[0:-1])) # recall that `columns` returns a list of column names"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Now the `timestamp` column is the first column: \ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Inspect the DataFrame's schema, note that `timestamp` is indeed classed as a timestamp\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# If you miss the time component columns,\n# you can get them back! Use the `year()`,\n# `month()`, `dayofmonth()`, `hour()`, and\n# `minute()` functions with `withColumn()`\ndisplay(\n  nycflights\n  .withColumn('year', year(nycflights.timestamp))\n  .withColumn('month', month(nycflights.timestamp))\n  .withColumn('day', dayofmonth(nycflights.timestamp))\n  .withColumn('hour', hour(nycflights.timestamp))\n  .withColumn('minute', minute(nycflights.timestamp))\n)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["There are more time-based functions:\n- `date_sub()`: subtract an integer number of days from a *Date* or *Timestamp*\n- `date_add()`: add an integer number of days from a *Date* or *Timestamp*\n- `datediff()`: get the difference between two dates\n- `add_months()`: add an integer number of months\n- `months_between()`: get the number of months between two dates\n- `next_day()`: returns the first date which is later than the value of the date column\n- `last_day()`: returns the last day of the month which the given date belongs to\n- `dayofmonth()`: extract the day of the month of a given date as integer\n- `dayofyear()`: extract the day of the year of a given date as integer\n- `weekofyear()`: extract the week number of a given date as integer\n- `quarter()`: extract the quarter of a given date"],"metadata":{}},{"cell_type":"code","source":["# Let's transform the timestamp in the first\n# record of `nycflights` with each of these\n# functions\ndisplay(\n  nycflights\n   .limit(1)\n   .select('timestamp')\n   .withColumn('date_sub', date_sub(nycflights.timestamp, 7))\n   .withColumn('date_add', date_add(nycflights.timestamp, 7))\n   .withColumn('datediff', datediff(nycflights.timestamp, nycflights.timestamp))\n   .withColumn('add_months', add_months(nycflights.timestamp, 1))\n   .withColumn('months_between', months_between(nycflights.timestamp, nycflights.timestamp))\n   .withColumn('next_day', next_day(nycflights.timestamp, 'Mon'))\n   .withColumn('last_day', last_day(nycflights.timestamp))\n   .withColumn('dayofmonth', dayofmonth(nycflights.timestamp))\n   .withColumn('dayofyear', dayofyear(nycflights.timestamp))\n   .withColumn('weekofyear', weekofyear(nycflights.timestamp))\n   .withColumn('quarter', quarter(nycflights.timestamp))\n   )"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["###Joins"],"metadata":{}},{"cell_type":"markdown","source":["Joins are easily performed with Spark DataFrames. The expression is:\n\n`join(other, on = None, how = None)`\n\nwhere:\n- other: a DataFrame that serves as the right side of the join\n- on: typically a join expression\n- how: the default is `inner` but there are also `inner`, `outer`, `left_outer`, `right_outer`, and `leftsemi` joins available"],"metadata":{}},{"cell_type":"markdown","source":["Let's load in some more data so that we can have two DataFrames to join. The **CSV** file `weather.csv` contains hourly meteorological data from EWR during 2013."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object...\nweather_schema = StructType([  \n  StructField('year', IntegerType(), True),\n  StructField('month', IntegerType(), True),\n  StructField('day', IntegerType(), True),\n  StructField('hour', IntegerType(), True),\n  StructField('temp', FloatType(), True),\n  StructField('dewp', FloatType(), True),\n  StructField('humid', FloatType(), True),\n  StructField('wind_dir', IntegerType(), True),\n  StructField('wind_speed', FloatType(), True),\n  StructField('wind_gust', FloatType(), True),\n  StructField('precip', FloatType(), True),\n  StructField('pressure', FloatType(), True),\n  StructField('visib', FloatType(), True)\n  ])\n\n#...and then read the CSV with the schema\nweather = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(weather_schema)\n .options(header = True)\n .load('/mnt/data-files/weather.csv'))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(weather)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# We need those `month`, `day`, and `hour` values back\nnycflights = \\\n(nycflights\n .withColumn('month', month(nycflights.timestamp))\n .withColumn('day', dayofmonth(nycflights.timestamp))\n .withColumn('hour', hour(nycflights.timestamp)))\n\n# Join the `nycflights` DF with the `weather` DF \nnycflights_all_columns = \\\n(nycflights\n .join(weather,\n       [nycflights.month == weather.month, # three join conditions: month,\n        nycflights.day == weather.day,     #                        day,\n        nycflights.hour == weather.hour],  #                        hour\n       'left_outer')) # left outer join: keep all rows from the left DF (flights), with the matching rows in the right DF (weather)\n                      # NULLs created if there is no match to the right DF"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# Notice that lots of columns created, as well as duplicate column names (not a bug! a feature?)\ndisplay(nycflights_all_columns)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# One way to reduce the number of extraneous\n# columns is to use a `select()` statement\nnycflights_wind_visib = \\\n(nycflights_all_columns\n .select(['timestamp', 'carrier', 'flight',\n          'origin', 'dest', 'wind_dir',\n          'wind_speed', 'wind_gust', 'visib']))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# Examine the DataFrame, now with less columns\ndisplay(nycflights_wind_visib)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Let's load in even more data so we can determine if any takeoffs occurred in very windy weather.\n\nThe **CSV** `beaufort_land.csv` contains Beaufort scale values (the `force` column), wind speed ranges in *mph*, and the name for each wind force."],"metadata":{}},{"cell_type":"code","source":["# Create a schema object... \nbeaufort_land_schema = StructType([  \n  StructField('force', IntegerType(), True),\n  StructField('speed_mi_h_lb', IntegerType(), True),\n  StructField('speed_mi_h_ub', IntegerType(), True),\n  StructField('name', StringType(), True)\n  ])\n\n# ...and then read the CSV with the schema\nbeaufort_land = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(beaufort_land_schema)\n .options(header = True)\n .load('/mnt/data-files/beaufort_land.csv'))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(beaufort_land)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# Join the current working DF with the `beaufort_land` DF\n# and use join expressions that use the WS ranges\nnycflights_wind_visib_beaufort = \\\n(nycflights_wind_visib\n .join(beaufort_land,\n      [nycflights_wind_visib.wind_speed >= beaufort_land.speed_mi_h_lb,\n       nycflights_wind_visib.wind_speed < beaufort_land.speed_mi_h_ub],\n       'left_outer')\n .withColumn('month', month(nycflights_wind_visib.timestamp)) # Create a month column from `timestamp` values\n .drop('speed_mi_h_lb')\n .drop('speed_mi_h_ub')\n)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["# View the joined DF; now we have extra data on wind speed!\ndisplay(nycflights_wind_visib_beaufort)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# We can inspect the number of potentially dangerous\n# takeoffs (i.e., where the Beaufort force is high)\n# month-by-month through the use of the `crosstab()` function\ncrosstab_month_force = \\\n(nycflights_wind_visib_beaufort\n .crosstab('month', 'force'))\n\n# After creating the crosstab DataFrame, use a few\n# functions to clean up the resultant DataFrame\ncrosstab_month_force = \\\n(crosstab_month_force\n .withColumn('month_force',\n             crosstab_month_force.month_force.cast('int')) # the column is initially a string but recasting as\n                                                           # an `int` will aid ordering in the next expression\n .orderBy('month_force')\n .drop('null'))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# Display the cross tabulation; turns out January was a bit riskier for takeoffs due to wind conditions\ndisplay(crosstab_month_force)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["###User Defined Functions (UDFs)\n\n**UDF**s allow for computations of values while looking at every input row in the DataFrame. They allow you to make your own function and import functionality from other **Python** libraries."],"metadata":{}},{"cell_type":"code","source":["# Define a function to convert velocity from\n# miles per hour (mph) to meters per second (mps)\ndef mph_to_mps(mph):\n  mps = mph * 0.44704\n  return mps\n\n# Register this function as a UDF using `udf()`\nmph_to_mps = udf(mph_to_mps, FloatType()) # An output type was specified"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# Create two new columns that are conversions of wind\n# speeds from mph to mps\ndisplay(\n  nycflights_wind_visib_beaufort\n  .withColumn('wind_speed_mps', mph_to_mps('wind_speed'))\n  .withColumn('wind_gust_mps', mph_to_mps('wind_gust'))\n  .withColumnRenamed('wind_speed', 'wind_speed_mph')\n  .withColumnRenamed('wind_gust', 'wind_gust_mph')\n)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["###Writing DataFrames to Files\nWe can easily write DataFrame data to a variety of different file formats."],"metadata":{}},{"cell_type":"code","source":["# Saving to CSV is quite similar to reading from a CSV file\n(crosstab_month_force\n .write\n .mode('overwrite')\n .format('com.databricks.spark.csv')\n .save('/mnt/spark-atp/nycflights13/crosstab_month_force/'))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["# Saving to Parquet is generally recommended for later retrieval\n(crosstab_month_force\n .write\n .mode('overwrite')\n .parquet('/mnt/spark-atp/nycflights13/crosstab_month_force_parquet/'))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["###Useful Links"],"metadata":{}},{"cell_type":"markdown","source":["There are many more functions... although I tried to cover a lot of ground, there are dozens more functions for DataFrames that I haven't touched upon.\n\nThe main project page for Spark:\n\n- http://spark.apache.org\n\nThe main reference for PySpark is:\n\n- http://spark.apache.org/docs/latest/api/python/index.html\n\nThese examples are available at:\n\n- https://github.com/rich-iannone/so-many-pyspark-examples\n\nInformation on the Parquet file format can be found at its project page:\n\n- http://parquet.apache.org\n\nThe GitHub project page for `spark-csv` package; contains usage documentation:\n\n- https://github.com/databricks/spark-csv"],"metadata":{}}],"metadata":{"name":"spark-dataframes","notebookId":96485},"nbformat":4,"nbformat_minor":0}
